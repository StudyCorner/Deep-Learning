{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6g1G19XvIJis"
      },
      "source": [
        "\n",
        "Recurrent Neural Networks (RNN) are to the rescue when the sequence of information is needed to be captured (another use case may include Time Series, next word prediction, etc.). Due to its internal memory factor, it remembers past sequences along with current input which makes it capable to capture context rather than just individual words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-2ncGawIQOF"
      },
      "source": [
        "We will conduct Sentiment Analysis to understand text classification using Tensorflow!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mscRvYbKISfJ"
      },
      "source": [
        "**Importing Libraries and Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_YCMuqAIQsH"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import SimpleRNN, LSTM, GRU, Bidirectional, Dense, Embedding\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.models import Sequential\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-w2HaqOZIXkV"
      },
      "source": [
        "We will be using Keras IMDB dataset. vocabulary size is a parameter that is used the get data containing the given number of most occurring words in the entire corpus of textual data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3QYG2AOIZ1a",
        "outputId": "90103324-6104-45c9-c38f-b24a971ff325"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 0s 0us/step\n",
            "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n"
          ]
        }
      ],
      "source": [
        "# Getting reviews with words that come under 5000\n",
        "# most occurring words in the entire\n",
        "# corpus of textual review data\n",
        "vocab_size = 5000\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
        "\n",
        "print(x_train[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvTP31iOIYQa"
      },
      "source": [
        "These are the index values of the words and hence we done see any reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Bp3HqGIIhr3",
        "outputId": "b7367500-0943-44cf-b8ef-23703cbe9081"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1641221/1641221 [==============================] - 0s 0us/step\n",
            "['the', 'as', 'you', 'with', 'out', 'themselves', 'powerful', 'lets', 'loves', 'their', 'becomes', 'reaching', 'had', 'journalist', 'of', 'lot', 'from', 'anyone', 'to', 'have', 'after', 'out', 'atmosphere', 'never', 'more', 'room', 'and', 'it', 'so', 'heart', 'shows', 'to', 'years', 'of', 'every', 'never', 'going', 'and', 'help', 'moments', 'or', 'of', 'every', 'chest', 'visual', 'movie', 'except', 'her', 'was', 'several', 'of', 'enough', 'more', 'with', 'is', 'now', 'current', 'film', 'as', 'you', 'of', 'mine', 'potentially', 'unfortunately', 'of', 'you', 'than', 'him', 'that', 'with', 'out', 'themselves', 'her', 'get', 'for', 'was', 'camp', 'of', 'you', 'movie', 'sometimes', 'movie', 'that', 'with', 'scary', 'but', 'and', 'to', 'story', 'wonderful', 'that', 'in', 'seeing', 'in', 'character', 'to', 'of', '70s', 'and', 'with', 'heart', 'had', 'shadows', 'they', 'of', 'here', 'that', 'with', 'her', 'serious', 'to', 'have', 'does', 'when', 'from', 'why', 'what', 'have', 'critics', 'they', 'is', 'you', 'that', \"isn't\", 'one', 'will', 'very', 'to', 'as', 'itself', 'with', 'other', 'and', 'in', 'of', 'seen', 'over', 'and', 'for', 'anyone', 'of', 'and', 'br', \"show's\", 'to', 'whether', 'from', 'than', 'out', 'themselves', 'history', 'he', 'name', 'half', 'some', 'br', 'of', 'and', 'odd', 'was', 'two', 'most', 'of', 'mean', 'for', '1', 'any', 'an', 'boat', 'she', 'he', 'should', 'is', 'thought', 'and', 'but', 'of', 'script', 'you', 'not', 'while', 'history', 'he', 'heart', 'to', 'real', 'at', 'and', 'but', 'when', 'from', 'one', 'bit', 'then', 'have', 'two', 'of', 'script', 'their', 'with', 'her', 'nobody', 'most', 'that', 'with', \"wasn't\", 'to', 'with', 'armed', 'acting', 'watch', 'an', 'for', 'with', 'and', 'film', 'want', 'an']\n"
          ]
        }
      ],
      "source": [
        "# Getting all the words from word_index dictionary\n",
        "word_idx = imdb.get_word_index()\n",
        "\n",
        "# Originally the index number of a value and not a key,\n",
        "# hence converting the index as key and the words as values\n",
        "word_idx = {i: word for word, i in word_idx.items()}\n",
        "\n",
        "# again printing the review\n",
        "print([word_idx[i] for i in x_train[0]])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z88U7KpZIf8c"
      },
      "source": [
        "Letâ€™s check the range of the reviews we have in this dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S09Et7YrIljR",
        "outputId": "2e6a47a3-66ab-4d45-a9f9-3e63e21bceda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max length of a review::  2697\n",
            "Min length of a review::  70\n"
          ]
        }
      ],
      "source": [
        "# Get the minimum and the maximum length of reviews\n",
        "print(\"Max length of a review:: \", len(max((x_train+x_test), key=len)))\n",
        "print(\"Min length of a review:: \", len(min((x_train+x_test), key=len)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRq-7ubyIptJ"
      },
      "source": [
        "We see that the longest review available is 2697 words and the shortest one is 70. While working with Neural Networks, it is important to make all the inputs in a fixed size. To achieve this objective we will pad the review sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aqHnT7IIq5q"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing import sequence\n",
        "\n",
        "# Keeping a fixed length of all reviews to max 400 words\n",
        "max_words = 400\n",
        "\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=max_words)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=max_words)\n",
        "\n",
        "x_valid, y_valid = x_train[:64], y_train[:64]\n",
        "x_train_, y_train_ = x_train[64:], y_train[64:]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7QjR5WOIqNP"
      },
      "source": [
        "**SimpleRNN (also called Vanilla RNN)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnSWDiXEIysk"
      },
      "source": [
        "They are the most basic form of Recurrent Neural Networks that tries to memorize sequential information. However, they have the native problems of Exploding and Vanishing gradients. For a detailed understanding of how RNNs works and its limitations please read the article Recurrent Neural Networks Explanation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2fgD4FaIzUm",
        "outputId": "74790c46-2a88-4b56-f27f-63bb64a007d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"Simple_RNN\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 400, 32)           160000    \n",
            "                                                                 \n",
            " simple_rnn (SimpleRNN)      (None, 128)               20608     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 180737 (706.00 KB)\n",
            "Trainable params: 180737 (706.00 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "390/390 [==============================] - 67s 168ms/step - loss: 0.6760 - accuracy: 0.5644 - val_loss: 0.5224 - val_accuracy: 0.7500\n",
            "Epoch 2/5\n",
            "390/390 [==============================] - 66s 169ms/step - loss: 0.5746 - accuracy: 0.6990 - val_loss: 0.6723 - val_accuracy: 0.5938\n",
            "Epoch 3/5\n",
            "390/390 [==============================] - 64s 164ms/step - loss: 0.5068 - accuracy: 0.7530 - val_loss: 0.6034 - val_accuracy: 0.7188\n",
            "Epoch 4/5\n",
            "390/390 [==============================] - 65s 166ms/step - loss: 0.4591 - accuracy: 0.7863 - val_loss: 0.6201 - val_accuracy: 0.8125\n",
            "Epoch 5/5\n",
            "390/390 [==============================] - 64s 165ms/step - loss: 0.4769 - accuracy: 0.7673 - val_loss: 0.7289 - val_accuracy: 0.5469\n",
            "\n",
            "Simple_RNN Score--->  [0.6325091123580933, 0.6265599727630615]\n"
          ]
        }
      ],
      "source": [
        "# fixing every word's embedding size to be 32\n",
        "embd_len = 32\n",
        "\n",
        "# Creating a RNN model\n",
        "RNN_model = Sequential(name=\"Simple_RNN\")\n",
        "RNN_model.add(Embedding(vocab_size,\n",
        "\t\t\t\t\t\tembd_len,\n",
        "\t\t\t\t\t\tinput_length=max_words))\n",
        "\n",
        "# In case of a stacked(more than one layer of RNN)\n",
        "# use return_sequences=True\n",
        "RNN_model.add(SimpleRNN(128,\n",
        "\t\t\t\t\t\tactivation='tanh',\n",
        "\t\t\t\t\t\treturn_sequences=False))\n",
        "RNN_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# printing model summary\n",
        "print(RNN_model.summary())\n",
        "\n",
        "# Compiling model\n",
        "RNN_model.compile(\n",
        "\tloss=\"binary_crossentropy\",\n",
        "\toptimizer='adam',\n",
        "\tmetrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Training the model\n",
        "history = RNN_model.fit(x_train_, y_train_,\n",
        "\t\t\t\t\t\tbatch_size=64,\n",
        "\t\t\t\t\t\tepochs=5,\n",
        "\t\t\t\t\t\tverbose=1,\n",
        "\t\t\t\t\t\tvalidation_data=(x_valid, y_valid))\n",
        "\n",
        "# Printing model score on test data\n",
        "print()\n",
        "print(\"Simple_RNN Score---> \", RNN_model.evaluate(x_test, y_test, verbose=0))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOdpPAgEI5ni"
      },
      "source": [
        "The vanilla form of RNN gave us a Test Accuracy of 64.95%. Limitations of Simple RNN are it is unable to handle long sentences well because of its vanishing gradient problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQ5_PNdqI8So"
      },
      "source": [
        "**Gated Recurrent Units (GRU)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGV7LehLI99n"
      },
      "source": [
        "GRUs are lesser know but equally robust algorithms to solve the limitations of simple RNNs. Please read the article Gated Recurrent Unit Networks for a better understanding of their work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0X8dF9AI_rw",
        "outputId": "50639155-5719-4594-afbb-b6772f554256"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"GRU_Model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 400, 32)           160000    \n",
            "                                                                 \n",
            " gru (GRU)                   (None, 128)               62208     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 222337 (868.50 KB)\n",
            "Trainable params: 222337 (868.50 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "390/390 [==============================] - 262s 665ms/step - loss: 0.5071 - accuracy: 0.7378 - val_loss: 0.2647 - val_accuracy: 0.8906\n",
            "Epoch 2/5\n",
            "390/390 [==============================] - 252s 647ms/step - loss: 0.2947 - accuracy: 0.8800 - val_loss: 0.2155 - val_accuracy: 0.9219\n",
            "Epoch 3/5\n",
            "390/390 [==============================] - 259s 663ms/step - loss: 0.2424 - accuracy: 0.9042 - val_loss: 0.2247 - val_accuracy: 0.9219\n",
            "Epoch 4/5\n",
            "390/390 [==============================] - 257s 660ms/step - loss: 0.1979 - accuracy: 0.9242 - val_loss: 0.1831 - val_accuracy: 0.9688\n",
            "Epoch 5/5\n",
            "390/390 [==============================] - 257s 660ms/step - loss: 0.1552 - accuracy: 0.9431 - val_loss: 0.2678 - val_accuracy: 0.9062\n",
            "\n",
            "GRU model Score--->  [0.33523690700531006, 0.8739200234413147]\n"
          ]
        }
      ],
      "source": [
        "# Defining GRU model\n",
        "gru_model = Sequential(name=\"GRU_Model\")\n",
        "gru_model.add(Embedding(vocab_size,\n",
        "\t\t\t\t\t\tembd_len,\n",
        "\t\t\t\t\t\tinput_length=max_words))\n",
        "gru_model.add(GRU(128,\n",
        "\t\t\t\tactivation='tanh',\n",
        "\t\t\t\treturn_sequences=False))\n",
        "gru_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Printing the Summary\n",
        "print(gru_model.summary())\n",
        "\n",
        "# Compiling the model\n",
        "gru_model.compile(\n",
        "\tloss=\"binary_crossentropy\",\n",
        "\toptimizer='adam',\n",
        "\tmetrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Training the GRU model\n",
        "history2 = gru_model.fit(x_train_, y_train_,\n",
        "\t\t\t\t\t\tbatch_size=64,\n",
        "\t\t\t\t\t\tepochs=5,\n",
        "\t\t\t\t\t\tverbose=1,\n",
        "\t\t\t\t\t\tvalidation_data=(x_valid, y_valid))\n",
        "\n",
        "# Printing model score on test data\n",
        "print()\n",
        "print(\"GRU model Score---> \", gru_model.evaluate(x_test, y_test, verbose=0))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Knr_RXtqI-XK"
      },
      "source": [
        "Test Accuracy of GRU was found to be 88.14%. GRU is a form of RNN that are better than simple RNN and are often faster than LSTM due to its relatively fewer training parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IagQDq1uJDzH"
      },
      "source": [
        "**Long Short Term Memory (LSTM)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acj7cZsQJIGN"
      },
      "source": [
        "LSTM is better in terms of capturing the memory of sequential information better than simple RNNs. To understand the theoretical aspects of LSTM please visit the article Long Short Term Memory Networks Explanation. Due to increased complexity than that of GRU, it is slower to train but in general, LSTMs give better accuracy than GRUs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "F16fiHl5JKGF",
        "outputId": "6a9e8967-185b-4c0b-e90e-f169898ca5db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"LSTM_Model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 400, 32)           160000    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 128)               82432     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 242561 (947.50 KB)\n",
            "Trainable params: 242561 (947.50 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "390/390 - 245s - loss: nan - accuracy: 0.5016 - val_loss: nan - val_accuracy: 0.6094 - 245s/epoch - 628ms/step\n",
            "Epoch 2/5\n",
            "390/390 - 247s - loss: nan - accuracy: 0.4997 - val_loss: nan - val_accuracy: 0.6094 - 247s/epoch - 633ms/step\n",
            "Epoch 3/5\n",
            "390/390 - 245s - loss: nan - accuracy: 0.4997 - val_loss: nan - val_accuracy: 0.6094 - 245s/epoch - 627ms/step\n",
            "Epoch 4/5\n",
            "390/390 - 246s - loss: nan - accuracy: 0.4997 - val_loss: nan - val_accuracy: 0.6094 - 246s/epoch - 630ms/step\n",
            "Epoch 5/5\n",
            "390/390 - 245s - loss: nan - accuracy: 0.4997 - val_loss: nan - val_accuracy: 0.6094 - 245s/epoch - 628ms/step\n",
            "\n",
            "LSTM model Score--->  [nan, 0.5]\n"
          ]
        }
      ],
      "source": [
        "# Defining LSTM model\n",
        "lstm_model = Sequential(name=\"LSTM_Model\")\n",
        "lstm_model.add(Embedding(vocab_size,\n",
        "\t\t\t\t\t\tembd_len,\n",
        "\t\t\t\t\t\tinput_length=max_words))\n",
        "lstm_model.add(LSTM(128,\n",
        "\t\t\t\t\tactivation='relu',\n",
        "\t\t\t\t\treturn_sequences=False))\n",
        "lstm_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Printing Model Summary\n",
        "print(lstm_model.summary())\n",
        "\n",
        "# Compiling the model\n",
        "lstm_model.compile(\n",
        "\tloss=\"binary_crossentropy\",\n",
        "\toptimizer='adam',\n",
        "\tmetrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Training the model\n",
        "history3 = lstm_model.fit(x_train_, y_train_,\n",
        "\t\t\t\t\t\tbatch_size=64,\n",
        "\t\t\t\t\t\tepochs=5,\n",
        "\t\t\t\t\t\tverbose=2,\n",
        "\t\t\t\t\t\tvalidation_data=(x_valid, y_valid))\n",
        "\n",
        "# Displaying the model accuracy on test data\n",
        "print()\n",
        "print(\"LSTM model Score---> \", lstm_model.evaluate(x_test, y_test, verbose=0))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuUSxGUBJIzx"
      },
      "source": [
        "LSTM model Provided a test accuracy of 81.95%.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zWLSR21JVWv"
      },
      "source": [
        "**Bi-directional LSTM Model**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dt2AeAgNJYFt"
      },
      "source": [
        "Bidirectional LSTMS are a derivative of traditional LSTMS. Here, two LSTMs are used to capture both the forward and backward sequences of the input. This helps in capturing the context better than normal LSTM. For more information on Bidirectional LSTM please read the article Emotion Detection using Bidirectional LSTM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArT9myAJJZ27",
        "outputId": "bd61daea-5d89-4667-d6ca-4ddecc905336"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"Bidirectional_LSTM\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, 400, 32)           160000    \n",
            "                                                                 \n",
            " bidirectional (Bidirection  (None, 256)               164864    \n",
            " al)                                                             \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 325121 (1.24 MB)\n",
            "Trainable params: 325121 (1.24 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "390/390 - 711s - loss: 0.4941 - accuracy: 0.7495 - val_loss: 0.3469 - val_accuracy: 0.8542 - 711s/epoch - 2s/step\n",
            "Epoch 2/5\n",
            "390/390 - 705s - loss: 0.3150 - accuracy: 0.8731 - val_loss: 0.3119 - val_accuracy: 0.8682 - 705s/epoch - 2s/step\n",
            "Epoch 3/5\n",
            "390/390 - 653s - loss: 0.2486 - accuracy: 0.9033 - val_loss: 0.3113 - val_accuracy: 0.8709 - 653s/epoch - 2s/step\n",
            "Epoch 4/5\n",
            "390/390 - 701s - loss: 0.2289 - accuracy: 0.9113 - val_loss: 0.3316 - val_accuracy: 0.8664 - 701s/epoch - 2s/step\n",
            "Epoch 5/5\n",
            "390/390 - 704s - loss: 0.1924 - accuracy: 0.9280 - val_loss: 0.3280 - val_accuracy: 0.8703 - 704s/epoch - 2s/step\n",
            "\n",
            "Bidirectional LSTM model Score--->  [0.327996164560318, 0.8703200221061707]\n"
          ]
        }
      ],
      "source": [
        "# Defining Bidirectional LSTM model\n",
        "bi_lstm_model = Sequential(name=\"Bidirectional_LSTM\")\n",
        "bi_lstm_model.add(Embedding(vocab_size,\n",
        "\t\t\t\t\t\t\tembd_len,\n",
        "\t\t\t\t\t\t\tinput_length=max_words))\n",
        "bi_lstm_model.add(Bidirectional(LSTM(128,\n",
        "\t\t\t\t\t\t\t\t\tactivation='tanh',\n",
        "\t\t\t\t\t\t\t\t\treturn_sequences=False)))\n",
        "bi_lstm_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Printing model summary\n",
        "print(bi_lstm_model.summary())\n",
        "\n",
        "# Compiling model summary\n",
        "bi_lstm_model.compile(\n",
        "loss=\"binary_crossentropy\",\n",
        "optimizer='adam',\n",
        "metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Training the model\n",
        "history4 = bi_lstm_model.fit(x_train_, y_train_,\n",
        "\t\t\t\t\t\t\tbatch_size=64,\n",
        "\t\t\t\t\t\t\tepochs=5,\n",
        "\t\t\t\t\t\t\tverbose=2,\n",
        "\t\t\t\t\t\t\tvalidation_data=(x_test, y_test))\n",
        "\n",
        "# Printing model score on test data\n",
        "print()\n",
        "print(\"Bidirectional LSTM model Score---> \",\n",
        "\tbi_lstm_model.evaluate(x_test, y_test, verbose=0))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdWaLezqJYfK"
      },
      "source": [
        "Bidirectional LSTM gave a test score of 87.48%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Tke5LAhJd25"
      },
      "source": [
        "**Conclusion**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQb6EgvDJiBA"
      },
      "source": [
        "All the major flavors for Recurrent Neural Networks were tested in their base forms keeping all the common hyperparameters like number of layers, activation function, batch size, and epochs to be the same across all the above models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6IALMn0Jk7g"
      },
      "source": [
        "The model complexity increases as we go from SimpleRNN to Bidirectional LSTM as the number of trainable parameters goes up.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Amz9O21hJmNU"
      },
      "source": [
        "Out of all the models, for the given dataset of IMDB reviews, the GRU model gave the best result in terms of accuracy."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}